{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading faiss.\n"
     ]
    }
   ],
   "source": [
    "# to prevent to restart kernel when any changes are made to any imported file\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# to import any file from some other directory\n",
    "# sys.path.append(\"/tmp/fastai/old\")\n",
    "\n",
    "# to stop printing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# to increase cells width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# to enable collapsible Headings and Functions\n",
    "# !pip install jupyter_contrib_nbextensions\n",
    "# !jupyter contrib nbextension install --user\n",
    "# !jupyter nbextensions_configurator enable --user\n",
    "# !jupyter nbextension enable codefolding/main\n",
    "# search collapsible to enable\n",
    "\n",
    "# enable dark theme\n",
    "# !pip install jupyterthemes\n",
    "# !jt -t monokai\n",
    "\n",
    "# monokai\n",
    "# solarizedd\n",
    "# !jt -r\n",
    "import numpy as np\n",
    "import faiss\n",
    "from faiss import normalize_L2\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import json\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os.path, time\n",
    "from datetime import datetime, timedelta\n",
    "from threading import Timer\n",
    "import pdb, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 209 word vectors. 1\n"
     ]
    }
   ],
   "source": [
    "from get_glove_emb import get_word2vec_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5) (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "\n",
    "PATH=\"../real_not_real_kaggle/data/\"\n",
    "train=pd.read_csv(f\"{PATH}train.csv\").reset_index(drop=True)\n",
    "test=pd.read_csv(f\"{PATH}test.csv\").reset_index(drop=True)\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "# train[\"key\"] = [\"KEY\"+x for x in list(map(str, random.sample(range(1000000, 9999999), len(train))))]\n",
    "# test[\"key\"]  = [\"KEY\"+x for x in list(map(str, random.sample(range(1000000, 9999999), len(test))))]\n",
    "train[\"key\"] = \"KEY\"+train.index.astype(str)\n",
    "test[\"key\"]  = \"KEY\"+test.index.astype(str)\n",
    "\n",
    "# tr_embeddings = get_word2vec_embeddings(train, \"tokens\")\n",
    "# ts_embeddings = get_word2vec_embeddings(test, \"tokens\")\n",
    "# train[\"emb\"]=list(tr_embeddings)\n",
    "# test[\"emb\"]=list(ts_embeddings)\n",
    "\n",
    "# print(np.array(tr_embeddings).shape)\n",
    "# print(np.array(ts_embeddings).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "      <td>KEY0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>KEY1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>\n",
       "      <td>1</td>\n",
       "      <td>KEY2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "      <td>KEY3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "      <td>KEY4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location  \\\n",
       "0  1   NaN     NaN       \n",
       "1  4   NaN     NaN       \n",
       "2  5   NaN     NaN       \n",
       "3  6   NaN     NaN       \n",
       "4  7   NaN     NaN       \n",
       "\n",
       "                                                                                                                                    text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all                                                                   \n",
       "1  Forest fire near La Ronge Sask. Canada                                                                                                  \n",
       "2  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected   \n",
       "3  13,000 people receive #wildfires evacuation orders in California                                                                        \n",
       "4  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school                                                 \n",
       "\n",
       "   target   key  \n",
       "0  1       KEY0  \n",
       "1  1       KEY1  \n",
       "2  1       KEY2  \n",
       "3  1       KEY3  \n",
       "4  1       KEY4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test[\"key\"].sample(200)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3263, 5), 3263)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, test.key.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_pickle(\"/home/cloud_user/faisal/data/data_latest\")\n",
    "# df=df.rename(columns={\"number\": \"key\", \"merged\": \"text\"})\n",
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_save_index(df, ids_col, xb_faiss_col, indx_fn, ids_fn):\n",
    "    try:\n",
    "        \n",
    "        ids=np.array(df[ids_col].str.slice(start=3).tolist()).astype(int)\n",
    "        xb_faiss = np.float32(np.asarray(df[xb_faiss_col].tolist()))\n",
    "\n",
    "        d  = xb_faiss.shape[1] \n",
    "        print(d, type(d))\n",
    "\n",
    "        normalize_L2(xb_faiss)\n",
    "\n",
    "        index_faiss = faiss.IndexIDMap(faiss.IndexFlatIP(d))\n",
    "        index_faiss.add_with_ids(xb_faiss, ids)\n",
    "\n",
    "        print(index_faiss.is_trained)\n",
    "        print(index_faiss.ntotal)\n",
    "\n",
    "        faiss.write_index(index_faiss, indx_fn)\n",
    "        np.save(ids_fn, ids)\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        fname =os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "        print(\"Exception get_similar_tickets: \", e, exc_type, fname, exc_tb.tb_lineno)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "def avg_feature_vector(sentence, num_features, word_index):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, word_index[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=pd.read_pickle(f\"{PATH}glove.840B.300d.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196008"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"emb\"] = train[\"text\"].apply(lambda x: avg_feature_vector(x, 300, word_index)) \n",
    "test[\"emb\"]  = test[\"text\"].apply(lambda x: avg_feature_vector(x, 300, word_index)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"emb\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Gensen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "create_save_index(df.sample(1000), \"key\", \"emb\", \"index_gensen\", \"ids_gensen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result = get_similar_tickets(df.sample(2)[[\"key\", \"emb\", \"text\"]], k=5, threshold=0.95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 <class 'int'>\n",
      "True\n",
      "7613\n"
     ]
    }
   ],
   "source": [
    "index_faiss=create_save_index(train, \"key\", \"emb\", \"index_gloveee\", \"ids_gloveee\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "7613\n"
     ]
    }
   ],
   "source": [
    "from similar_git import get_similar_tickets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function called\n",
      "starting calculations\n",
      "embedding col already present\n",
      "query vector created !\n",
      "Here we go ! <class 'faiss.swigfaiss.IndexIDMap'> (10, 300)\n",
      "Got results from FAISS\n",
      "create result dataframe\n",
      "Overall total time:  0.165050211828202\n"
     ]
    }
   ],
   "source": [
    "s=timeit.default_timer()\n",
    "mydf = get_similar_tickets(test.sample(10)[[\"key\", \"emb\", \"text\"]], k=5, threshold=0.95)\n",
    "e=timeit.default_timer()\n",
    "print(\"Overall total time: \", e-s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>query_text</th>\n",
       "      <th>NN1_text</th>\n",
       "      <th>NN1_score</th>\n",
       "      <th>NN1_number</th>\n",
       "      <th>NN2_text</th>\n",
       "      <th>NN2_score</th>\n",
       "      <th>NN2_number</th>\n",
       "      <th>NN3_text</th>\n",
       "      <th>NN3_score</th>\n",
       "      <th>NN3_number</th>\n",
       "      <th>NN4_text</th>\n",
       "      <th>NN4_score</th>\n",
       "      <th>NN4_number</th>\n",
       "      <th>NN5_text</th>\n",
       "      <th>NN5_score</th>\n",
       "      <th>NN5_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KEY1967</td>\n",
       "      <td>#EPAO MARSAC reports on Manipur flood: Heavy incessant rains in Manipur have led to inundation causin... http://t.co/rh7m7T9Gd9 #MANIPUR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KEY2602</td>\n",
       "      <td>A sinkhole grows ... in Brooklyn? http://t.co/UiqKDdVwKz http://t.co/aa8FwtOHyJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KEY3062</td>\n",
       "      <td>Medieval Upheaval (Hardy Boys: The Secret Files)\\nFranklin W. Dixon - Aladdin. http://t.co/Yb7ijKtlnB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KEY575</td>\n",
       "      <td>hermancranston: WIRED : All these fires are burning through firefighters' budgets http://t.co/FSGAnfRgjH http://t.co/ju9zzu9TiOÛ_ Û_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KEY1509</td>\n",
       "      <td>#Fracking #Ecocide Eyewitness to Extreme Weather: 11 Social Media Posts that Show Just How Crazy T... http://t.co/dEdDH8Rme8 #Revolution</td>\n",
       "      <td>#ClimateChange Eyewitness to Extreme Weather: 11 Social Media Posts that Show Just How Crazy Things A... http://t.co/czpDn9oBiT #Anarchy</td>\n",
       "      <td>0.969349</td>\n",
       "      <td>KEY3516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KEY183</td>\n",
       "      <td>Mourning notices for stabbing arson victims stir Û÷politics of griefÛª in Israel: Posters for Shira Banki and A... http://t.co/6o92wDfcLu</td>\n",
       "      <td>Mourning notices for stabbing arson victims stir Û÷politics of griefÛª in Israel: Posters for Shira Banki and A... http://t.co/3GZ5zQQTHe</td>\n",
       "      <td>1</td>\n",
       "      <td>KEY397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KEY2459</td>\n",
       "      <td>To All The Meat-Loving Feminists Of The World Riot Grill Has Arrived http://t.co/MK9ZIp2ro7</td>\n",
       "      <td>To All The Meat-Loving Feminists Of The World Riot Grill Has Arrived http://t.co/TiOst8oKvX</td>\n",
       "      <td>1</td>\n",
       "      <td>KEY5758</td>\n",
       "      <td>To All The Meat-Loving Feminists Of The World Riot Grill Has Arrived http://t.co/SkAAUSjpO4 OliviaMiles01</td>\n",
       "      <td>1</td>\n",
       "      <td>KEY5766</td>\n",
       "      <td>To All The Meat-Loving Feminists Of The World Riot Grill HasåÊArrived http://t.co/uDQA53KfQu</td>\n",
       "      <td>0.973201</td>\n",
       "      <td>KEY5747</td>\n",
       "      <td>To All The Meat-Loving Feminists Of The World Riot Grill Has Arrived http://t.co/um3wTL5r7K #arts http://t.co/2LQyxZQ5DN</td>\n",
       "      <td>0.972885</td>\n",
       "      <td>KEY5745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KEY2406</td>\n",
       "      <td>Refugees as citizens - The Hindu http://t.co/GJSaAf3U6K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KEY917</td>\n",
       "      <td>Microsoft Xbox 360 console RRoD red ring of death AS IS FOR PARTS OR REPAIR - Full read byÛ_ http://t.co/IpQCCT5hGC http://t.co/oofPEfRh3r</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KEY1376</td>\n",
       "      <td>Leaving back to SF Friday have not packed one single thing 911 emergency</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key  \\\n",
       "0  KEY1967   \n",
       "1  KEY2602   \n",
       "2  KEY3062   \n",
       "3  KEY575    \n",
       "4  KEY1509   \n",
       "5  KEY183    \n",
       "6  KEY2459   \n",
       "7  KEY2406   \n",
       "8  KEY917    \n",
       "9  KEY1376   \n",
       "\n",
       "                                                                                                                                    query_text  \\\n",
       "0  #EPAO MARSAC reports on Manipur flood: Heavy incessant rains in Manipur have led to inundation causin... http://t.co/rh7m7T9Gd9 #MANIPUR      \n",
       "1  A sinkhole grows ... in Brooklyn? http://t.co/UiqKDdVwKz http://t.co/aa8FwtOHyJ                                                               \n",
       "2  Medieval Upheaval (Hardy Boys: The Secret Files)\\nFranklin W. Dixon - Aladdin. http://t.co/Yb7ijKtlnB                                         \n",
       "3  hermancranston: WIRED : All these fires are burning through firefighters' budgets http://t.co/FSGAnfRgjH http://t.co/ju9zzu9TiOÛ_ Û_        \n",
       "4  #Fracking #Ecocide Eyewitness to Extreme Weather: 11 Social Media Posts that Show Just How Crazy T... http://t.co/dEdDH8Rme8 #Revolution      \n",
       "5  Mourning notices for stabbing arson victims stir Û÷politics of griefÛª in Israel: Posters for Shira Banki and A... http://t.co/6o92wDfcLu   \n",
       "6  To All The Meat-Loving Feminists Of The World Riot Grill Has Arrived http://t.co/MK9ZIp2ro7                                                   \n",
       "7  Refugees as citizens - The Hindu http://t.co/GJSaAf3U6K                                                                                       \n",
       "8  Microsoft Xbox 360 console RRoD red ring of death AS IS FOR PARTS OR REPAIR - Full read byÛ_ http://t.co/IpQCCT5hGC http://t.co/oofPEfRh3r   \n",
       "9  Leaving back to SF Friday have not packed one single thing 911 emergency                                                                      \n",
       "\n",
       "                                                                                                                                      NN1_text  \\\n",
       "0  NaN                                                                                                                                           \n",
       "1  NaN                                                                                                                                           \n",
       "2  NaN                                                                                                                                           \n",
       "3  NaN                                                                                                                                           \n",
       "4  #ClimateChange Eyewitness to Extreme Weather: 11 Social Media Posts that Show Just How Crazy Things A... http://t.co/czpDn9oBiT #Anarchy      \n",
       "5  Mourning notices for stabbing arson victims stir Û÷politics of griefÛª in Israel: Posters for Shira Banki and A... http://t.co/3GZ5zQQTHe   \n",
       "6  To All The Meat-Loving Feminists Of The World Riot Grill Has Arrived http://t.co/TiOst8oKvX                                                   \n",
       "7  NaN                                                                                                                                           \n",
       "8  NaN                                                                                                                                           \n",
       "9  NaN                                                                                                                                           \n",
       "\n",
       "  NN1_score NN1_number  \\\n",
       "0  NaN       NaN         \n",
       "1  NaN       NaN         \n",
       "2  NaN       NaN         \n",
       "3  NaN       NaN         \n",
       "4  0.969349  KEY3516     \n",
       "5  1         KEY397      \n",
       "6  1         KEY5758     \n",
       "7  NaN       NaN         \n",
       "8  NaN       NaN         \n",
       "9  NaN       NaN         \n",
       "\n",
       "                                                                                                    NN2_text  \\\n",
       "0  NaN                                                                                                         \n",
       "1  NaN                                                                                                         \n",
       "2  NaN                                                                                                         \n",
       "3  NaN                                                                                                         \n",
       "4  NaN                                                                                                         \n",
       "5  NaN                                                                                                         \n",
       "6  To All The Meat-Loving Feminists Of The World Riot Grill Has Arrived http://t.co/SkAAUSjpO4 OliviaMiles01   \n",
       "7  NaN                                                                                                         \n",
       "8  NaN                                                                                                         \n",
       "9  NaN                                                                                                         \n",
       "\n",
       "  NN2_score NN2_number  \\\n",
       "0  NaN       NaN         \n",
       "1  NaN       NaN         \n",
       "2  NaN       NaN         \n",
       "3  NaN       NaN         \n",
       "4  NaN       NaN         \n",
       "5  NaN       NaN         \n",
       "6  1         KEY5766     \n",
       "7  NaN       NaN         \n",
       "8  NaN       NaN         \n",
       "9  NaN       NaN         \n",
       "\n",
       "                                                                                       NN3_text  \\\n",
       "0  NaN                                                                                            \n",
       "1  NaN                                                                                            \n",
       "2  NaN                                                                                            \n",
       "3  NaN                                                                                            \n",
       "4  NaN                                                                                            \n",
       "5  NaN                                                                                            \n",
       "6  To All The Meat-Loving Feminists Of The World Riot Grill HasåÊArrived http://t.co/uDQA53KfQu   \n",
       "7  NaN                                                                                            \n",
       "8  NaN                                                                                            \n",
       "9  NaN                                                                                            \n",
       "\n",
       "  NN3_score NN3_number  \\\n",
       "0  NaN       NaN         \n",
       "1  NaN       NaN         \n",
       "2  NaN       NaN         \n",
       "3  NaN       NaN         \n",
       "4  NaN       NaN         \n",
       "5  NaN       NaN         \n",
       "6  0.973201  KEY5747     \n",
       "7  NaN       NaN         \n",
       "8  NaN       NaN         \n",
       "9  NaN       NaN         \n",
       "\n",
       "                                                                                                                   NN4_text  \\\n",
       "0  NaN                                                                                                                        \n",
       "1  NaN                                                                                                                        \n",
       "2  NaN                                                                                                                        \n",
       "3  NaN                                                                                                                        \n",
       "4  NaN                                                                                                                        \n",
       "5  NaN                                                                                                                        \n",
       "6  To All The Meat-Loving Feminists Of The World Riot Grill Has Arrived http://t.co/um3wTL5r7K #arts http://t.co/2LQyxZQ5DN   \n",
       "7  NaN                                                                                                                        \n",
       "8  NaN                                                                                                                        \n",
       "9  NaN                                                                                                                        \n",
       "\n",
       "  NN4_score NN4_number  NN5_text NN5_score NN5_number  \n",
       "0  NaN       NaN       NaN        NaN       NaN        \n",
       "1  NaN       NaN       NaN        NaN       NaN        \n",
       "2  NaN       NaN       NaN        NaN       NaN        \n",
       "3  NaN       NaN       NaN        NaN       NaN        \n",
       "4  NaN       NaN       NaN        NaN       NaN        \n",
       "5  NaN       NaN       NaN        NaN       NaN        \n",
       "6  0.972885  KEY5745   NaN        NaN       NaN        \n",
       "7  NaN       NaN       NaN        NaN       NaN        \n",
       "8  NaN       NaN       NaN        NaN       NaN        \n",
       "9  NaN       NaN       NaN        NaN       NaN        "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    vals=mydf[\"NN\"+str(i)+\"_number\"].tolist()\n",
    "    if pd.isnull(vals).all():\n",
    "        mydf[\"NN\"+str(i)+\"_text\"]=np.nan\n",
    "    else:\n",
    "        mydf[\"NN\"+str(i)+\"_text\"] = train.set_index(\"key\").loc[mydf[\"NN\"+str(i)+\"_number\"].tolist()].text.tolist()\n",
    "final=mydf[[\"key\", \"query_text\", \"NN1_text\",\"NN1_score\", \"NN1_number\", \"NN2_text\",\"NN2_score\", \"NN2_number\", \"NN3_text\",\"NN3_score\", \"NN3_number\", \"NN4_text\",\"NN4_score\", \"NN4_number\", \"NN5_text\",\"NN5_score\",\"NN5_number\", ]]\n",
    "final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue in script:\n",
    "#     test with gensen embeddings [Issues not in embeddings]\n",
    "\n",
    "# issue in embeddings:\n",
    "#     test with some other embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def get_coefs(word,*arr): \n",
    "#     return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "# def load_embedding(file):\n",
    "#     if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "#         embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "#     else:\n",
    "#         embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "#     return embeddings_index\n",
    "# def make_embedding_matrix(embedding, tokenizer, len_voc):\n",
    "#     all_embs = np.stack(embedding.values())\n",
    "#     emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "#     embed_size = all_embs.shape[1]\n",
    "#     word_index = tokenizer.word_index\n",
    "#     embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "    \n",
    "#     for word, i in word_index.items():\n",
    "#         if i >= len_voc:\n",
    "#             continue\n",
    "#         embedding_vector = embedding.get(word)\n",
    "#         if embedding_vector is not None: \n",
    "#             embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "#     return embedding_matrix\n",
    "# PATH=\"../real_not_real_kaggle/data/\"\n",
    "\n",
    "\n",
    "# glove = load_embedding(f'../real_not_real_kaggle/data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# glove_dir = f'{PATH}/' # This is the folder with the dataset\n",
    "\n",
    "# glove_embedding = {} # We create a dictionary of word -> embedding\n",
    "# f = open(f'{PATH}glove.6B.50d.txt') # Open file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_voc = 10000\n",
    "# # Tokenizing\n",
    "# def make_tokenizer(texts, len_voc):\n",
    "#     from keras.preprocessing.text import Tokenizer\n",
    "#     t = Tokenizer(num_words=len_voc)\n",
    "#     t.fit_on_texts(texts)\n",
    "#     return t\n",
    "# tokenizer = make_tokenizer(train['text'], len_voc)\n",
    "# # Using TensorFlow backend.\n",
    "# X = tokenizer.texts_to_sequences(train['text'])\n",
    "# # I also apply padding, mostly to store X as an array.\n",
    "\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# X = pad_sequences(X, 70)\n",
    "# y = df['target'].values\n",
    "# # For visualization, I'm gonna need to see which index corresponds to which word\n",
    "\n",
    "# index_word = {0: ''}\n",
    "# for word in tokenizer.word_index.keys():\n",
    "#     index_word[tokenizer.word_index[word]] = word\n",
    "# # Embedding Matrix\n",
    "# embed_mat = make_embedding_matrix(glove, tokenizer, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from nltk.util import ngrams\n",
    "\n",
    "# stop=set(stopwords.words('english'))\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# df=train.copy()\n",
    "\n",
    "\n",
    "\n",
    "# def create_corpus(df):\n",
    "#     corpus=[]\n",
    "#     for tweet in tqdm(df['text']):\n",
    "#         words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "#         corpus.append(words)\n",
    "#     return corpus\n",
    "        \n",
    "        \n",
    "# corpus=create_corpus(df)\n",
    "# # 100%|██████████| 10876/10876 [00:03<00:00, 3606.29it/s]\n",
    "# embedding_dict={}\n",
    "# with open(f'../real_not_real_kaggle/data/glove.6B.50d.txt','r') as f:\n",
    "#     try:\n",
    "#         for line in f:\n",
    "#                 values=line.split()\n",
    "#                 word=values[0]\n",
    "#                 vectors=np.asarray(values[1:],'float32')\n",
    "#                 embedding_dict[word]=vectors\n",
    "#     except:\n",
    "#         pass\n",
    "# f.close()\n",
    "# MAX_LEN=50\n",
    "# print(\"step-1\")\n",
    "# tokenizer_obj=Tokenizer()\n",
    "# print(\"step-2\")\n",
    "\n",
    "\n",
    "# tokenizer_obj.fit_on_texts(corpus)\n",
    "# print(\"step-3\")\n",
    "\n",
    "\n",
    "# sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "# print(\"step-4\")\n",
    "\n",
    "\n",
    "# tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "# word_index=tokenizer_obj.word_index\n",
    "# print('Number of unique words:',len(word_index))\n",
    "# # Number of unique words: 20342\n",
    "# num_words=len(word_index)+1\n",
    "# embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "# for word,i in tqdm(word_index.items()):\n",
    "#     if i > num_words:\n",
    "#         continue\n",
    "    \n",
    "#     emb_vec=embedding_dict.get(word)\n",
    "#     if emb_vec is not None:\n",
    "#         embedding_matrix[i]=emb_vec\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 25\n",
    "embedding_size = 200\n",
    "df=train.copy()\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(df['headline']))\n",
    "X = tokenizer.texts_to_sequences(df['headline'])\n",
    "X = pad_sequences(X, maxlen = maxlen)\n",
    "y = df['is_sarcastic']\n",
    "# Using TensorFlow backend.\n",
    "# EMBEDDING_FILE = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\n",
    "EMBEDDING_FILE = '../real_not_real_kaggle/data/glove.6B.50d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
